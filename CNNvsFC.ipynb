{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vmlsa88cqmib"
      },
      "source": [
        "## Exercise 0: Train your model on GPU (0 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfirXJR3qvB0"
      },
      "source": [
        "For some tasks in this assignment, it can take a long time if you run it on CPU. For example, based on our test on Exercise 3 Task 4, it will take roughly 2 hours to train the full model for 1 epoch on CPU. Hence, we highly recommend you try to train your model on GPU.\n",
        "\n",
        "To do so, first you need to enable GPU on Colab (this will restart the runtime). Click `Runtime`-> `Change runtime type` and select the `Hardware accelerator` there.  You can then run the following code to see if the GPU is correctly initialized and available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT7wKvilvBvX",
        "outputId": "3e1b1d1b-0b0f-469f-f2b1-af93c475d332"
      },
      "source": [
        "import torch\n",
        "print(f'Can I can use GPU now? -- {torch.cuda.is_available()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can I can use GPU now? -- True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnPPDoA7vQWp"
      },
      "source": [
        "### You must manually move your model and data to the GPU (and sometimes back to the cpu)\n",
        "After setting the GPU up on colab, then you should put your **model** and **data** to GPU. We give a simple example below. You can use `to` function for this task. See [torch.Tensor.to](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html) to move a tensor to the GPU (probably your mini-batch of data in each iteration) or [torch.nn.Module.to](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to) to move your NN model to GPU (assuming you create subclass [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)). Note that `to()` of tensor returns a NEW tensor while `to` of a NN model will apply this in-place. To be safe, the best semantics are `obj = obj.to(device)`. For printing, you will need to move a tensor back to the CPU via the `cpu()` function.\n",
        "\n",
        "Once the model and input data are on the GPU, everything else can be done the same.  This is the beauty of PyTorch GPU acceleration.  None of the other code needs to be altered.\n",
        "\n",
        "To summarize, you need to 1) enable GPU acceleration in Colab, 2) put the model on the GPU, and 3) put the input data (i.e., the batch of samples) onto the GPU using `to()` after it is loaded by the data loaders (usually you only put one batch of data on the GPU at a time)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUdR8v7Kt-6S",
        "outputId": "acd6bb7a-f146-42e2-c2bc-8d61601f1bc2"
      },
      "source": [
        "import torch.nn as nn\n",
        "rand_tensor = torch.rand(5,2)\n",
        "simple_model = nn.Sequential(nn.Linear(2,10), nn.ReLU(), nn.Linear(10,1))\n",
        "print(f'input is on {rand_tensor.device}')\n",
        "print(f'model parameters are on {[param.device for param in simple_model.parameters()]}')\n",
        "print(f'output is on {simple_model(rand_tensor).device}')\n",
        "\n",
        "device = torch.device('cuda')\n",
        "# ----------- <Your code> ---------------\n",
        "# Move rand_tensor and model onto the GPU device\n",
        "rand_tensor = rand_tensor.to(device)\n",
        "simple_model = simple_model.to(device)\n",
        "# --------- <End your code> -------------\n",
        "print(f'input is on {rand_tensor.device}')\n",
        "print(f'model parameters are on {[param.device for param in simple_model.parameters()]}')\n",
        "print(f'output is on {simple_model(rand_tensor).device}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input is on cpu\n",
            "model parameters are on [device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu')]\n",
            "output is on cpu\n",
            "input is on cuda:0\n",
            "model parameters are on [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
            "output is on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz2T0QYYpwVR"
      },
      "source": [
        "## Exercise 1: Why use a CNN rather than only fully connected layers? (30 points)\n",
        "\n",
        "In this exercise, you will build two models for the **MNIST** dataset: one uses only fully connected layers and another uses a standard CNN layout (convolution layers everywhere except the last layer is fully connected layer). The two models should be built with roughly the same accuracy performance, your task is to compare the number of network parameters (a huge number of parameters can affect training/testing time, memory requirements, overfitting, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFKqew7r-E-q"
      },
      "source": [
        "### Task 1: Following the structure used in the instructions, you should create\n",
        "\n",
        "*   One network named **OurFC** which should consist with only fully connected layers\n",
        "\n",
        "  *   You should decide how many layers and how many hidden dimensions you want in your network \n",
        "  *   Your final accuracy on the test dataset should lie roughly around 90% ($\\pm$2%)\n",
        "  *   There is no need to make the neural network unnecessarily complex, your total training time should no longer than 3 mins\n",
        "\n",
        "*   Another network named **OurCNN** which applys a standard CNN structure\n",
        "  *   Again, you should decide how many layers and how many channels you want for each layer.\n",
        "  *   Your final accuracy on the test dataset should lie roughly around 90% ($\\pm$2%)\n",
        "  *   A standard CNN structure can be composed as **[Conv2d, MaxPooling, ReLU] x num_conv_layers + FC x num_fc_layers**\n",
        "\n",
        "* Train and test your network on MNIST data as in the instructions\n",
        "* You are **required** to print out the loss in the training and loss+accuracy in the test as in the instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIVO5T4JgA_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb8d9b6-3b22-4697-f8bf-1285c56a6c21"
      },
      "source": [
        "# Import MNIST data, pre-process it and split it to test and train set\n",
        "import torchvision\n",
        "\n",
        "train_batch_size, test_batch_size = 64, 1000\n",
        "\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                                           torchvision.transforms.Normalize((0.1307,),(0.3081,))]) # MNIST data has 0.1307 mean and 0.3081 standard deviation\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST('/data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST('/data', train=False, download=True, transform=transform)\n",
        "\n",
        "print(f'Training Data: {train_dataset}')\n",
        "print(f'Testing Data: {test_dataset}')\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, train_batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, test_batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data: Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: /data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
            "           )\n",
            "Testing Data: Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: /data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6k6Ccw1K6se"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Fully Connected Network Class\n",
        "class OurFC(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(OurFC, self).__init__()\n",
        "    self.fc = nn.Linear(784, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, x.size()[2]*x.size()[3])\n",
        "    x = self.fc(x)\n",
        "    return F.log_softmax(x, -1)\n",
        "\n",
        "# CNN Network Class\n",
        "class OurCNN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(OurCNN, self).__init__()\n",
        "    self.conv = nn.Conv2d(1, 1, kernel_size=5) # Using only 1 filter to match the accuracy ~92%, otherwise it shoots to 95%\n",
        "    self.fc = nn.Linear(144, 10)\n",
        "  \n",
        "  def forward(self, x):\n",
        "      x = self.conv(x)\n",
        "      x = F.relu(F.max_pool2d(x, 2))\n",
        "      x = x.view(-1, x.size()[1]*x.size()[2]*x.size()[3])\n",
        "      x = self.fc(x)\n",
        "      return F.log_softmax(x, -1)\n",
        "\n",
        "FCClassifier = OurFC()\n",
        "FCOptimizer = optim.SGD(FCClassifier.parameters(), lr=0.01, momentum=0.7)\n",
        "\n",
        "CNNClassifier = OurCNN()\n",
        "CNNOptimizar = optim.SGD(CNNClassifier.parameters(), lr=0.01, momentum=0.7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-a6mpvCRrgx"
      },
      "source": [
        "import time\n",
        "\n",
        "# Function to train the given classifier using the given optimizer\n",
        "def train(classifier, optimizer, epoch, train_loss, train_counter):\n",
        "  start_time = time.time()\n",
        "  for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = classifier(images)\n",
        "    loss = F.nll_loss(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx%10 == 0:\n",
        "      train_loss.append(loss.item())\n",
        "      train_counter.append(train_batch_size*batch_idx + (epoch-1)*len(train_loader.dataset))\n",
        "    if batch_idx%100 == 0:\n",
        "      print(f'Epoch: {epoch} [{batch_idx*len(images)}/{len(train_loader.dataset)}] Training Loss: {loss.item()}')\n",
        "  print(f'Total training time for epoch: {epoch} is {time.time()-start_time} seconds')\n",
        "\n",
        "# Function to test the given classifier using the given optimizer\n",
        "def test(classifier, optimizer, epoch, test_loss, test_counter):\n",
        "  classifier.eval()\n",
        "\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, targets in test_loader:\n",
        "      outputs = classifier(images)\n",
        "      loss += F.nll_loss(outputs, targets, reduction='sum').item()\n",
        "      prediction = outputs.data.max(1, keepdim=True)[1]\n",
        "      correct += prediction.eq(targets.data.view_as(prediction)).sum()\n",
        "\n",
        "  test_loss.append(loss/len(test_loader.dataset))\n",
        "  test_counter.append(len(train_loader.dataset)*epoch)\n",
        "  print(f'Test Result on epoch: {epoch}: Avg loss is {loss/len(test_loader.dataset)}, Accuracy: {100*(correct/len(test_loader.dataset))}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6i84CTrQ72M",
        "outputId": "4a06e31e-80ec-4d61-8b2a-e4ad14179f64"
      },
      "source": [
        "FC_train_loss, FC_train_counter = [], []\n",
        "FC_test_loss, FC_test_counter = [], []\n",
        "CNN_train_loss, CNN_train_counter = [], []\n",
        "CNN_test_loss, CNN_test_counter = [], []\n",
        "max_epoch = 3\n",
        "\n",
        "# Function to train and test the model\n",
        "def train_and_test(classifier, optimizer, train_loss, train_counter, test_loss, test_counter):\n",
        "  for epoch in range(1, max_epoch+1):\n",
        "    train(classifier, optimizer, epoch, train_loss, train_counter)\n",
        "    test(classifier, optimizer, epoch, test_loss, test_counter)\n",
        "\n",
        "print(f'>>>>>>>>>>>>>>Printing Results for FC Netowrk (OurFC)<<<<<<<<<<<<<<')\n",
        "train_and_test(FCClassifier, FCOptimizer, FC_train_loss, FC_train_counter, FC_test_loss, FC_test_counter)\n",
        "print(f'>>>>>>>>>>>>>>Printing Results for CNN Netowrk (OurCNN)<<<<<<<<<<<<<<')\n",
        "train_and_test(CNNClassifier, CNNOptimizar, CNN_train_loss, CNN_train_counter, CNN_test_loss, CNN_test_counter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>>>>>>>>>>>>Printing Results for FC Netowrk (OurFC)<<<<<<<<<<<<<<\n",
            "Epoch: 1 [0/60000] Training Loss: 0.14794547855854034\n",
            "Epoch: 1 [6400/60000] Training Loss: 0.19575344026088715\n",
            "Epoch: 1 [12800/60000] Training Loss: 0.5624825358390808\n",
            "Epoch: 1 [19200/60000] Training Loss: 0.30876073241233826\n",
            "Epoch: 1 [25600/60000] Training Loss: 0.2982504069805145\n",
            "Epoch: 1 [32000/60000] Training Loss: 0.4672214090824127\n",
            "Epoch: 1 [38400/60000] Training Loss: 0.3649389147758484\n",
            "Epoch: 1 [44800/60000] Training Loss: 0.3104346990585327\n",
            "Epoch: 1 [51200/60000] Training Loss: 0.18228305876255035\n",
            "Epoch: 1 [57600/60000] Training Loss: 0.450327068567276\n",
            "Total training time for epoch: 1 is 12.64466118812561 seconds\n",
            "Test Result on epoch: 1: Avg loss is 0.27425271682739255, Accuracy: 92.2699966430664\n",
            "Epoch: 2 [0/60000] Training Loss: 0.45664820075035095\n",
            "Epoch: 2 [6400/60000] Training Loss: 0.35054245591163635\n",
            "Epoch: 2 [12800/60000] Training Loss: 0.2514461874961853\n",
            "Epoch: 2 [19200/60000] Training Loss: 0.3846513032913208\n",
            "Epoch: 2 [25600/60000] Training Loss: 0.24222132563591003\n",
            "Epoch: 2 [32000/60000] Training Loss: 0.21803636848926544\n",
            "Epoch: 2 [38400/60000] Training Loss: 0.10246366262435913\n",
            "Epoch: 2 [44800/60000] Training Loss: 0.18476277589797974\n",
            "Epoch: 2 [51200/60000] Training Loss: 0.18159249424934387\n",
            "Epoch: 2 [57600/60000] Training Loss: 0.1299528330564499\n",
            "Total training time for epoch: 2 is 12.506606340408325 seconds\n",
            "Test Result on epoch: 2: Avg loss is 0.27419463272094724, Accuracy: 92.18999481201172\n",
            "Epoch: 3 [0/60000] Training Loss: 0.16494719684123993\n",
            "Epoch: 3 [6400/60000] Training Loss: 0.255339652299881\n",
            "Epoch: 3 [12800/60000] Training Loss: 0.14023202657699585\n",
            "Epoch: 3 [19200/60000] Training Loss: 0.18589261174201965\n",
            "Epoch: 3 [25600/60000] Training Loss: 0.2707690894603729\n",
            "Epoch: 3 [32000/60000] Training Loss: 0.0937998965382576\n",
            "Epoch: 3 [38400/60000] Training Loss: 0.28497400879859924\n",
            "Epoch: 3 [44800/60000] Training Loss: 0.2453659474849701\n",
            "Epoch: 3 [51200/60000] Training Loss: 0.18892447650432587\n",
            "Epoch: 3 [57600/60000] Training Loss: 0.22537820041179657\n",
            "Total training time for epoch: 3 is 12.684024333953857 seconds\n",
            "Test Result on epoch: 3: Avg loss is 0.27526173706054685, Accuracy: 92.3499984741211\n",
            ">>>>>>>>>>>>>>Printing Results for CNN Netowrk (OurCNN)<<<<<<<<<<<<<<\n",
            "Epoch: 1 [0/60000] Training Loss: 0.17778676748275757\n",
            "Epoch: 1 [6400/60000] Training Loss: 0.2133992612361908\n",
            "Epoch: 1 [12800/60000] Training Loss: 0.3223446309566498\n",
            "Epoch: 1 [19200/60000] Training Loss: 0.37226277589797974\n",
            "Epoch: 1 [25600/60000] Training Loss: 0.42839017510414124\n",
            "Epoch: 1 [32000/60000] Training Loss: 0.2729247808456421\n",
            "Epoch: 1 [38400/60000] Training Loss: 0.3462747633457184\n",
            "Epoch: 1 [44800/60000] Training Loss: 0.13907940685749054\n",
            "Epoch: 1 [51200/60000] Training Loss: 0.20622745156288147\n",
            "Epoch: 1 [57600/60000] Training Loss: 0.5285042524337769\n",
            "Total training time for epoch: 1 is 16.66279101371765 seconds\n",
            "Test Result on epoch: 1: Avg loss is 0.2735957786560059, Accuracy: 91.93000030517578\n",
            "Epoch: 2 [0/60000] Training Loss: 0.14140890538692474\n",
            "Epoch: 2 [6400/60000] Training Loss: 0.28268149495124817\n",
            "Epoch: 2 [12800/60000] Training Loss: 0.3386811912059784\n",
            "Epoch: 2 [19200/60000] Training Loss: 0.2647782564163208\n",
            "Epoch: 2 [25600/60000] Training Loss: 0.2558378577232361\n",
            "Epoch: 2 [32000/60000] Training Loss: 0.37217143177986145\n",
            "Epoch: 2 [38400/60000] Training Loss: 0.3110010623931885\n",
            "Epoch: 2 [44800/60000] Training Loss: 0.33811241388320923\n",
            "Epoch: 2 [51200/60000] Training Loss: 0.2591574192047119\n",
            "Epoch: 2 [57600/60000] Training Loss: 0.24004681408405304\n",
            "Total training time for epoch: 2 is 16.65208125114441 seconds\n",
            "Test Result on epoch: 2: Avg loss is 0.27620182876586913, Accuracy: 91.82999420166016\n",
            "Epoch: 3 [0/60000] Training Loss: 0.18167909979820251\n",
            "Epoch: 3 [6400/60000] Training Loss: 0.15020212531089783\n",
            "Epoch: 3 [12800/60000] Training Loss: 0.4992900788784027\n",
            "Epoch: 3 [19200/60000] Training Loss: 0.27217909693717957\n",
            "Epoch: 3 [25600/60000] Training Loss: 0.12591750919818878\n",
            "Epoch: 3 [32000/60000] Training Loss: 0.16904550790786743\n",
            "Epoch: 3 [38400/60000] Training Loss: 0.4257957935333252\n",
            "Epoch: 3 [44800/60000] Training Loss: 0.27825111150741577\n",
            "Epoch: 3 [51200/60000] Training Loss: 0.5036069750785828\n",
            "Epoch: 3 [57600/60000] Training Loss: 0.1278022676706314\n",
            "Total training time for epoch: 3 is 16.696829795837402 seconds\n",
            "Test Result on epoch: 3: Avg loss is 0.2699896629333496, Accuracy: 92.12999725341797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fJY7m55fObt"
      },
      "source": [
        "### Task 2: Compare the number of parameters that are used in both your neural networks by printing out the total number of parameters for both of your networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEVoEH2QLvv0"
      },
      "source": [
        "**Note:** You need to clearly show which number corresponds to which network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrjN1dr_f-lc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11a88e2-4854-4eb3-8e8d-e354dc63db28"
      },
      "source": [
        "# Function to count number of learnable parameters in a model\n",
        "def count_num_of_parameters(model):\n",
        "  return sum(parameter.numel() for parameter in model.parameters() if parameter.requires_grad)\n",
        "\n",
        "print(f'OurFC Parameters: {count_num_of_parameters(FCClassifier)}')\n",
        "print(f'OurCNN Parameters: {count_num_of_parameters(CNNClassifier)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OurFC Parameters: 7850\n",
            "OurCNN Parameters: 1476\n"
          ]
        }
      ]
    }
  ]
}